{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自作LLM (Tokenizer編)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## byte pair encoding \n",
    "\n",
    "Suppose the data to be encoded is\n",
    "\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "11 token. \n",
    "\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:\n",
    "\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":\n",
    "\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":\n",
    "\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "5 token \n",
    "\n",
    "This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.\n",
    "\n",
    "To decompress the data, simply perform the replacements in the reverse order.\n",
    "\n",
    "[参考](https://en.wikipedia.org/wiki/Byte_pair_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping \"Python (programming language)\" as it already exists\n",
      "Skipping \"Attention Is All You Need\" as it already exists\n",
      "Skipping \"Harry Potter\" as it already exists\n",
      "Skipping \"The Big Bang Theory\" as it already exists\n",
      "File: ./training_data.txt has 148703 characters\n"
     ]
    }
   ],
   "source": [
    "# download text from wikipedia to make a dataset for training a language model\n",
    "import wikipedia as wiki\n",
    "import os\n",
    "import glob\n",
    "\n",
    "wiki.set_lang(\"en\")\n",
    "\n",
    "topics: list = [\"Python (programming language)\", \"Attention Is All You Need\",\"Harry Potter\", \"The Big Bang Theory\"]\n",
    "\n",
    "for topic in topics:\n",
    "    try:\n",
    "        if os.path.exists(\"data/{}.txt\".format(topic.replace(\" \", \"_\"))):\n",
    "            print(\"Skipping \\\"{}\\\" as it already exists\".format(topic))\n",
    "            continue\n",
    "        page = wiki.page(topic, auto_suggest=False)\n",
    "        content = page.content\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        with open(\"data/{}.txt\".format(topic.replace(\" \", \"_\")), \"w\") as f:\n",
    "            f.write(content)\n",
    "        print(\"Downloaded \\\"{}\\\"\".format(topic))\n",
    "    except:\n",
    "        print(\"Failed to download \\\"{}\\\"\".format(topic))\n",
    "        continue\n",
    "\n",
    "data_paths = glob.glob(\"data/*.txt\")\n",
    "training_data_path = os.path.join(\"./\",\"training_data.txt\")\n",
    "\n",
    "with open(training_data_path, \"w\") as f:\n",
    "    for path in data_paths:\n",
    "        with open(path, \"r\") as f2:\n",
    "            content = f2.read()\n",
    "            f.write(content)\n",
    "\n",
    "with open(training_data_path, \"r\") as f:\n",
    "    full_content = f.read()\n",
    "    print(\"File: {} has {} characters\".format(training_data_path, len(full_content)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unicode characters: 148854 characters\n"
     ]
    }
   ],
   "source": [
    "# translate the text to unicode\n",
    "unicode_text = full_content.encode(\"utf-8\")\n",
    "print(\"number of unicode characters: {} characters\".format(len(unicode_text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Radford, Alec, et al. \"Language models are unsupervised multitask learners.\" OpenAI blog 1.8 (2019): 9.](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "```\n",
    "Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and char- acter level inputs for infrequent symbol sequences. Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These imple- mentations would require including the full space of Uni- code symbols in order to model all Unicode strings. This would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added. This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE. In contrast, a byte-level version of BPE only requires a base vocabulary of size 256. However, directly applying BPE to the byte sequence results in sub- optimal merges due to BPE using a greedy frequency based heuristic for building the token vocabulary. We observed BPE including many versions of common words like dog since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we pre- vent BPE from merging across character categories for any byte sequence. We add an exception for spaces which sig- nificantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenizer_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
